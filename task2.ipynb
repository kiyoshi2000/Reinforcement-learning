{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f64ff1",
   "metadata": {},
   "source": [
    "# Policy Gradient on Racetrack Environment (Optimized)\n",
    "\n",
    "This notebook implements a Policy Gradient method for a custom racetrack environment using PyTorch. It includes:\n",
    "- Dynamic input size handling\n",
    "- Entropy-based regularization\n",
    "- Advantage normalization\n",
    "- Gradient clipping\n",
    "- **Performance optimization** by disabling rendering\n",
    "\n",
    "We use the REINFORCE algorithm, which is a Monte Carlo policy gradient method for optimizing the policy network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b3cec",
   "metadata": {},
   "source": [
    "### 1. Imports and Device Configuration\n",
    "\n",
    "We import the required libraries and set the device to CUDA if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b90b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Normal\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e7816",
   "metadata": {},
   "source": [
    "### 2. Training Mode and Render Settings\n",
    "\n",
    "We define a simple flag to control whether the environment should render for visualization or run headlessly for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86a84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True\n",
    "render_mode = None if training else \"human\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee153d",
   "metadata": {},
   "source": [
    "### 3. Environment Configuration\n",
    "\n",
    "The configuration dictionary defines both the observation and action spaces, along with various simulation and reward parameters. Key features include:\n",
    "- OccupancyGrid observation type with vehicle presence and road occupancy\n",
    "- Continuous action space with lateral-only control (no acceleration)\n",
    "- Reward structure penalizing collisions, drifting from the lane center, and overactive control\n",
    "- Rendering is disabled when training to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ae1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"OccupancyGrid\",\n",
    "        \"features\": ['presence', 'on_road', 'velocity', 'heading'],\n",
    "        \"grid_size\": [[-18, 18], [-18, 18]],\n",
    "        \"grid_step\": [2, 2],\n",
    "        \"as_image\": False,\n",
    "        \"align_to_vehicle_axes\": True\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\",\n",
    "        \"longitudinal\": True,\n",
    "        \"lateral\": True,\n",
    "    },\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"policy_frequency\": 10,\n",
    "    \"duration\": 100,\n",
    "    \"collision_reward\": -1,\n",
    "    \"lane_centering_cost\": 2,\n",
    "    # \"action_reward\": -0.05,\n",
    "    \"controlled_vehicles\": 1,\n",
    "    \"other_vehicles\": 0,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 600,\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"reward_speed_range\": [0.2, 0.8],\n",
    "    \"reward_speed_weight\": 0.2,\n",
    "    \"scaling\": 7,\n",
    "    \"show_trajectories\": False,\n",
    "    \"offroad_terminal\": True,\n",
    "    \"road\": {\n",
    "        \"type\": \"sine_curve\"},\n",
    "    \"render_agent\": False if training else True,\n",
    "    \"offscreen_rendering\": True if training else False,\n",
    "    \"offroad_terminal\": True,\n",
    "    \"offroad_reward\": -1.0  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0006b",
   "metadata": {},
   "source": [
    "### 4. Environment Initialization\n",
    "\n",
    "We instantiate the racetrack environment and inject the configuration directly into the unwrapped environment. A sample observation is retrieved to infer the shape of the state space and action dimensionality for the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d3bd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"racetrack-v0\", render_mode=render_mode)\n",
    "env.unwrapped.configure(config)\n",
    "obs_sample, _ = env.reset()\n",
    "\n",
    "obs_shape = obs_sample.shape\n",
    "act_dim = env.action_space.shape[0] if len(env.action_space.shape) > 0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b46710",
   "metadata": {},
   "source": [
    "### Policy and Value Networks\n",
    "\n",
    "We use a classic actor-critic architecture in which:\n",
    "- The Actor learns a stochastic policy modeled by a Normal distribution, allowing for continuous actions.\n",
    "- The Critic estimates the value function V(s), used to reduce the variance of the policy gradient via advantage estimation.\n",
    "- Both models use a fully connected feedforward network with ReLU activations and are designed to support dynamic input shapes.\n",
    "\n",
    "### Actor Network\n",
    "\n",
    "The actor outputs the parameters of a Normal distribution from which actions are sampled. We use log_std as a learnable parameter to allow the network to control its exploration behavior dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23f0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_shape, act_dim, hidden=128):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *obs_shape)\n",
    "            in_dim = self.flatten(dummy).shape[1]\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # cabeça de média\n",
    "        self.mu_head   = nn.Linear(hidden, act_dim)\n",
    "        # cabeça de log-std (inicializada em –1 ⇒ std≈0.37)\n",
    "        self.logstd_head = nn.Linear(hidden, act_dim)\n",
    "        nn.init.constant_(self.logstd_head.bias, -1.0)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x  = self.backbone(self.flatten(obs))\n",
    "        mu = torch.tanh(self.mu_head(x))        # [-1, 1]\n",
    "\n",
    "        log_std = self.logstd_head(x).clamp(-3, 1)  # std∈[0.05, 2.7]\n",
    "        std = log_std.exp()\n",
    "\n",
    "        return torch.distributions.Normal(mu, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27890fe1",
   "metadata": {},
   "source": [
    "### Critic Network\n",
    "\n",
    "The critic estimates the value of each state using a separate feedforward network. Its output is a scalar value for each state input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61819b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_shape, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *obs_shape)\n",
    "            n_flatten = self.flatten(dummy).shape[1]\n",
    "\n",
    "        self.v = nn.Sequential(\n",
    "            nn.Linear(n_flatten, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.flatten(obs)\n",
    "        return self.v(x).squeeze(-1)  # Output shape: (batch,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa299c",
   "metadata": {},
   "source": [
    "### Model Initialization and Optimizers\n",
    "\n",
    "We instantiate the networks and their optimizers. AdamW is used for better weight decay handling compared to standard Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a09ccadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "actor = Actor(obs_shape, act_dim).to(device)\n",
    "critic = Critic(obs_shape).to(device)\n",
    "\n",
    "# Optimizers\n",
    "opt_actor = optim.AdamW(actor.parameters(), lr=1e-3)\n",
    "opt_critic = optim.AdamW(critic.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d32d73",
   "metadata": {},
   "source": [
    "## REINFORCE with Baseline: Training Loop\n",
    "\n",
    "This section implements the training procedure for a policy gradient method (REINFORCE) using the actor-critic architecture defined earlier.\n",
    "\n",
    "Key elements of the implementation:\n",
    "- The actor samples actions from a Normal distribution.\n",
    "- The critic estimates the value function V(s) to compute the advantage.\n",
    "- Rewards are accumulated using Monte Carlo returns.\n",
    "- The policy is optimized using the advantage-weighted log-probabilities.\n",
    "- We include entropy regularization to encourage exploration.\n",
    "- Gradients are clipped to stabilize training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "439614ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep   0 | Return   21.1 | Ent 0.428\n",
      "Ep  10 | Return   14.5 | Ent 0.444\n",
      "Ep  20 | Return    3.9 | Ent 0.485\n"
     ]
    }
   ],
   "source": [
    "# ------ hiperparâmetros ------\n",
    "gamma   = 0.95\n",
    "epochs  = 100\n",
    "act_range = torch.tensor([5.0, 1.0], device=device)  # escala das ações (lon, lat)\n",
    "\n",
    "# ------ métricas ------\n",
    "ret_hist, len_hist, ent_hist = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    logps, values, rewards = [], [], []\n",
    "    done, step, ep_ret = False, 0, 0.0\n",
    "\n",
    "    while not done and step < 200:\n",
    "        dist  = actor(obs)\n",
    "        value = critic(obs)\n",
    "\n",
    "        raw_a = dist.rsample()                    # reparameterização\n",
    "        squashed_a = torch.tanh(raw_a)\n",
    "        env_a = (squashed_a * act_range).cpu().detach().numpy()[0]\n",
    "\n",
    "        # log-prob com correção do squash (ver Appendix de SAC)\n",
    "        logp = (dist.log_prob(raw_a) - torch.log(1 - squashed_a.pow(2) + 1e-6)).sum(-1)\n",
    "\n",
    "        obs_next, r, done, _, _ = env.step(env_a)\n",
    "\n",
    "        if hasattr(env, \"vehicle\") and not env.vehicle.on_road:\n",
    "            r -= 1.0;  done = True\n",
    "\n",
    "        logps.append(logp);  values.append(value);  rewards.append(r)\n",
    "        ep_ret += r\n",
    "        obs = torch.tensor(obs_next, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        step += 1\n",
    "\n",
    "    # -------- GAE simples --------\n",
    "    R, returns = 0.0, []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "    values  = torch.cat(values)\n",
    "    adv     = (returns - values)\n",
    "    adv     = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "    # -------- perdas --------\n",
    "    logps   = torch.cat(logps)\n",
    "    policy_loss = -(logps * adv.detach()).mean()\n",
    "    value_loss  = 0.5 * adv.pow(2).mean()\n",
    "    entropy     = dist.entropy().mean()\n",
    "    ent_hist.append(entropy.item())\n",
    "\n",
    "    ent_coef = 0.01 * (0.995 ** epoch)\n",
    "    loss = policy_loss + value_loss - ent_coef * entropy\n",
    "\n",
    "    # -------- otimização --------\n",
    "    opt_actor.zero_grad();  opt_critic.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), 0.5)\n",
    "    torch.nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
    "    opt_actor.step();  opt_critic.step()\n",
    "\n",
    "    ret_hist.append(ep_ret);  len_hist.append(step)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Ep {epoch:>3} | Return {ep_ret:>6.1f} | Ent {entropy:.3f}\")\n",
    "        \n",
    "# Salvar o checkpoint\n",
    "torch.save({\n",
    "    'actor_state_dict': actor.state_dict(),\n",
    "    'critic_state_dict': critic.state_dict(),\n",
    "    'opt_actor_state_dict': opt_actor.state_dict(),\n",
    "    'opt_critic_state_dict': opt_critic.state_dict(),\n",
    "    'epoch': epoch,\n",
    "}, 'models/checkpoint_task2.pth')\n",
    "\n",
    "# 📊 Plotagens\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(ret_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Recompensa por Episódio\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(ent_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Entropia\")\n",
    "plt.title(\"Entropia da Política\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(len_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Duração do Episódio\")\n",
    "plt.title(\"Passos por Episódio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb703707",
   "metadata": {},
   "source": [
    "### Rendering the Trained Agent\n",
    "\n",
    "After training the policy, we can visualize its performance by running the agent in the environment with rendering enabled. This helps qualitatively assess how well the agent learned the task, such as staying on track or making smooth lateral movements.\n",
    "\n",
    "#### Key Points\n",
    "- The environment is re-initialized with \"human\" render mode to display a GUI window.\n",
    "- The trained policy is used in deterministic mode by taking the mean of the action distribution.\n",
    "- Rendering is done for a limited number of steps or until the episode terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0be983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "actor = Actor(obs_shape, act_dim).to(device)\n",
    "critic = Critic(obs_shape).to(device)\n",
    "\n",
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=1e-4)\n",
    "opt_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "checkpoint = torch.load('models/checkpoint_task2.pth', map_location=device)\n",
    "\n",
    "actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "\n",
    "opt_actor.load_state_dict(checkpoint['opt_actor_state_dict'])\n",
    "opt_critic.load_state_dict(checkpoint['opt_critic_state_dict'])\n",
    "\n",
    "# Environment instantiation for rendering\n",
    "env_render = gym.make(\"racetrack-v0\", render_mode=\"human\")\n",
    "env_render.unwrapped.configure(config)\n",
    "\n",
    "obs, _ = env_render.reset()\n",
    "obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "while not done and step < 300:\n",
    "    dist = actor(obs)                                          # Forward pass\n",
    "    action = dist.mean                                          # Deterministic policy (mean of Normal)\n",
    "    obs_next, reward, done, truncated, _ = env_render.step(action.detach().cpu().numpy()[0])\n",
    "    \n",
    "    obs = torch.tensor(obs_next, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    step += 1\n",
    "\n",
    "env_render.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
