{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f64ff1",
   "metadata": {},
   "source": [
    "# Policy Gradient on Racetrack Environment (Optimized)\n",
    "\n",
    "This notebook implements a Policy Gradient method for a custom racetrack environment using PyTorch. It includes:\n",
    "- Dynamic input size handling\n",
    "- Entropy-based regularization\n",
    "- Advantage normalization\n",
    "- Gradient clipping\n",
    "- **Performance optimization** by disabling rendering\n",
    "\n",
    "We use the REINFORCE algorithm, which is a Monte Carlo policy gradient method for optimizing the policy network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b3cec",
   "metadata": {},
   "source": [
    "### 1. Imports and Device Configuration\n",
    "\n",
    "We import the required libraries and set the device to CUDA if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b90b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Normal\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e7816",
   "metadata": {},
   "source": [
    "### 2. Training Mode and Render Settings\n",
    "\n",
    "We define a simple flag to control whether the environment should render for visualization or run headlessly for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86a84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True\n",
    "render_mode = None if training else \"human\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee153d",
   "metadata": {},
   "source": [
    "### 3. Environment Configuration\n",
    "\n",
    "The configuration dictionary defines both the observation and action spaces, along with various simulation and reward parameters. Key features include:\n",
    "- OccupancyGrid observation type with vehicle presence and road occupancy\n",
    "- Continuous action space with lateral-only control (no acceleration)\n",
    "- Reward structure penalizing collisions, drifting from the lane center, and overactive control\n",
    "- Rendering is disabled when training to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76ae1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"OccupancyGrid\",\n",
    "        \"features\": ['presence','on_road','velocity','heading'],\n",
    "        \"grid_size\": [[-18,18],[-18,18]],\n",
    "        \"grid_step\": [2,2],\n",
    "        \"as_image\": False,\n",
    "        \"align_to_vehicle_axes\": True\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\",\n",
    "        \"longitudinal\": True,\n",
    "        \"lateral\": True,\n",
    "    },\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"policy_frequency\": 10,        # mais responsivo\n",
    "    \"duration\": 100,\n",
    "    \"collision_reward\": -5,        # colisões pesadas\n",
    "    \"offroad_terminal\": True,\n",
    "    \"offroad_reward\": -10.0,       # sai da pista = -10\n",
    "    \"lane_centering_cost\": 4,      # 2× mais penalização por desvio\n",
    "    \"action_reward\": -0.01,        # quase sem penalização de ação\n",
    "    \"reward_speed_range\": [0.4,1.0],\n",
    "    \"reward_speed_weight\": 0.5,\n",
    "    \"controlled_vehicles\": 1,\n",
    "    \"other_vehicles\": 0,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 600,\n",
    "    \"centering_position\": [0.5,0.5],\n",
    "    \"scaling\": 7,\n",
    "    \"show_trajectories\": False,\n",
    "    \"render_agent\": False if training else True,\n",
    "    \"offscreen_rendering\": True if training else False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0006b",
   "metadata": {},
   "source": [
    "### 4. Environment Initialization\n",
    "\n",
    "We instantiate the racetrack environment and inject the configuration directly into the unwrapped environment. A sample observation is retrieved to infer the shape of the state space and action dimensionality for the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d3bd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"racetrack-v0\", render_mode=render_mode)\n",
    "env.unwrapped.configure(config)\n",
    "obs_sample, _ = env.reset()\n",
    "\n",
    "obs_shape = obs_sample.shape\n",
    "act_dim = env.action_space.shape[0] if len(env.action_space.shape) > 0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b46710",
   "metadata": {},
   "source": [
    "### Policy and Value Networks\n",
    "\n",
    "We use a classic actor-critic architecture in which:\n",
    "- The Actor learns a stochastic policy modeled by a Normal distribution, allowing for continuous actions.\n",
    "- The Critic estimates the value function V(s), used to reduce the variance of the policy gradient via advantage estimation.\n",
    "- Both models use a fully connected feedforward network with ReLU activations and are designed to support dynamic input shapes.\n",
    "\n",
    "### Actor Network\n",
    "\n",
    "The actor outputs the parameters of a Normal distribution from which actions are sampled. We use log_std as a learnable parameter to allow the network to control its exploration behavior dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e23f0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, TransformedDistribution, transforms\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_shape, act_dim, hidden=128):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *obs_shape)\n",
    "            in_dim = self.flatten(dummy).shape[1]\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU()\n",
    "        )\n",
    "        self.mu_head     = nn.Linear(hidden, act_dim)\n",
    "        self.logstd_head = nn.Linear(hidden, act_dim)\n",
    "        nn.init.constant_(self.logstd_head.bias, -2.0)  # std≈0.14 inicial\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.backbone(self.flatten(obs))\n",
    "        mu      = self.mu_head(x)\n",
    "        log_std = self.logstd_head(x).clamp(-3, 1)  # limitação de std\n",
    "        std     = log_std.exp()\n",
    "        base_dist = Normal(mu, std)\n",
    "        # empilha o Tanh como transformação\n",
    "        dist = TransformedDistribution(base_dist, [transforms.TanhTransform(cache_size=1)])\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27890fe1",
   "metadata": {},
   "source": [
    "### Critic Network\n",
    "\n",
    "The critic estimates the value of each state using a separate feedforward network. Its output is a scalar value for each state input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61819b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_shape, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *obs_shape)\n",
    "            n_flatten = self.flatten(dummy).shape[1]\n",
    "\n",
    "        self.v = nn.Sequential(\n",
    "            nn.Linear(n_flatten, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.flatten(obs)\n",
    "        return self.v(x).squeeze(-1)  # Output shape: (batch,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa299c",
   "metadata": {},
   "source": [
    "### Model Initialization and Optimizers\n",
    "\n",
    "We instantiate the networks and their optimizers. AdamW is used for better weight decay handling compared to standard Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a09ccadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "actor = Actor(obs_shape, act_dim).to(device)\n",
    "critic = Critic(obs_shape).to(device)\n",
    "\n",
    "# Optimizers\n",
    "opt_actor = optim.AdamW(actor.parameters(), lr=1e-4)\n",
    "opt_critic = optim.AdamW(critic.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d32d73",
   "metadata": {},
   "source": [
    "## REINFORCE with Baseline: Training Loop\n",
    "\n",
    "This section implements the training procedure for a policy gradient method (REINFORCE) using the actor-critic architecture defined earlier.\n",
    "\n",
    "Key elements of the implementation:\n",
    "- The actor samples actions from a Normal distribution.\n",
    "- The critic estimates the value function V(s) to compute the advantage.\n",
    "- Rewards are accumulated using Monte Carlo returns.\n",
    "- The policy is optimized using the advantage-weighted log-probabilities.\n",
    "- We include entropy regularization to encourage exploration.\n",
    "- Gradients are clipped to stabilize training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "439614ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep   0 | Return   49.7 | Ent -0.566\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# ------ hiperparâmetros ------\n",
    "gamma   = 0.99\n",
    "epochs  = 200\n",
    "act_range = torch.tensor([5.0, 0.5], device=device)  # escala das ações (lon, lat)\n",
    "\n",
    "# ------ métricas ------\n",
    "ret_hist, len_hist, ent_hist = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    logps, values, rewards = [], [], []\n",
    "    done, step, ep_ret = False, 0, 0.0\n",
    "\n",
    "    while not done and step < 200:\n",
    "        value = critic(obs)\n",
    "\n",
    "        dist   = actor(obs)                      # TanhNormal\n",
    "        action = dist.rsample()                  # já em [-1,1]\n",
    "        logp   = dist.log_prob(action).sum(-1)   # com jacobiano incluído\n",
    "        env_a  = (action * act_range).cpu().detach().numpy()[0]\n",
    "\n",
    "        # log-prob com correção do squash (ver Appendix de SAC)\n",
    "        # logp = (dist.log_prob(raw_a) - torch.log(1 - squashed_a.pow(2) + 1e-6)).sum(-1)\n",
    "        logp = dist.log_prob(action).sum(-1) # Logp padrão da Normal\n",
    "\n",
    "        obs_next, r, terminated, truncated, _ = env.step(env_a)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        logps.append(logp);  values.append(value);  rewards.append(r)\n",
    "        ep_ret += r\n",
    "        obs = torch.tensor(obs_next, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        step += 1\n",
    "\n",
    "    # -------- GAE simples --------\n",
    "    R, returns = 0.0, []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "    values  = torch.cat(values)\n",
    "    adv     = (returns - values)\n",
    "    adv     = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "    # -------- perdas --------\n",
    "    logps = torch.cat(logps)\n",
    "    policy_loss = -(logps * adv.detach()).mean()\n",
    "    # value_loss = 0.5 * adv.pow(2).mean()\n",
    "    value_loss = F.mse_loss(values, returns)\n",
    "\n",
    "    entropy = dist.entropy().mean() # Média dos log-probs ao longo do episódio\n",
    "    ent_hist.append(entropy.item()) # Armazena o valor correto\n",
    "\n",
    "    ent_coef = 0.05 * (0.995 ** epoch)\n",
    "    loss = policy_loss + value_loss - ent_coef * entropy\n",
    "\n",
    "    # -------- otimização --------\n",
    "    opt_actor.zero_grad();  opt_critic.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), 0.5)\n",
    "    torch.nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
    "    opt_actor.step();  opt_critic.step()\n",
    "\n",
    "    ret_hist.append(ep_ret);  len_hist.append(step)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Ep {epoch:>3} | Return {ep_ret:>6.1f} | Ent {entropy:.3f}\")\n",
    "        \n",
    "# Salvar o checkpoint\n",
    "torch.save({\n",
    "    'actor_state_dict': actor.state_dict(),\n",
    "    'critic_state_dict': critic.state_dict(),\n",
    "    'opt_actor_state_dict': opt_actor.state_dict(),\n",
    "    'opt_critic_state_dict': opt_critic.state_dict(),\n",
    "    'epoch': epoch,\n",
    "}, 'models/checkpoint_task2.pth')\n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(ret_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Recompensa por Episódio\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(ent_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Entropia\")\n",
    "plt.title(\"Entropia da Política\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(len_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Duração do Episódio\")\n",
    "plt.title(\"Passos por Episódio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb703707",
   "metadata": {},
   "source": [
    "### Rendering the Trained Agent\n",
    "\n",
    "After training the policy, we can visualize its performance by running the agent in the environment with rendering enabled. This helps qualitatively assess how well the agent learned the task, such as staying on track or making smooth lateral movements.\n",
    "\n",
    "#### Key Points\n",
    "- The environment is re-initialized with \"human\" render mode to display a GUI window.\n",
    "- The trained policy is used in deterministic mode by taking the mean of the action distribution.\n",
    "- Rendering is done for a limited number of steps or until the episode terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0be983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "actor = Actor(obs_shape, act_dim).to(device)\n",
    "critic = Critic(obs_shape).to(device)\n",
    "\n",
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=1e-4)\n",
    "opt_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "checkpoint = torch.load('models/checkpoint_task2.pth', map_location=device)\n",
    "\n",
    "actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "\n",
    "opt_actor.load_state_dict(checkpoint['opt_actor_state_dict'])\n",
    "opt_critic.load_state_dict(checkpoint['opt_critic_state_dict'])\n",
    "\n",
    "# Environment instantiation for rendering\n",
    "env_render = gym.make(\"racetrack-v0\", render_mode=\"human\")\n",
    "env_render.unwrapped.configure(config)\n",
    "\n",
    "obs, _ = env_render.reset()\n",
    "obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "done = False\n",
    "step = 0\n",
    "max_steps_render = 500 # Aumente se necessário para ver mais\n",
    "\n",
    "# Certifique-se que act_range está definido aqui também\n",
    "act_range = torch.tensor([5.0, 0.5], device=device)\n",
    "\n",
    "while not done and step < max_steps_render: # Aumente o limite de passos se necessário\n",
    "    with torch.no_grad():\n",
    "        dist   = actor(obs)                # mesma dist com tanh embutido\n",
    "        action = dist.mean                  # valor médio (determinístico) já em [-1,1]\n",
    "        env_a   = (action * act_range).cpu().numpy()[0]\n",
    "\n",
    "    obs_next, reward, terminated, truncated, info = env_render.step(env_a)\n",
    "\n",
    "    # Use env_a agora\n",
    "    obs_next, reward, terminated, truncated, info = env_render.step(env_a.cpu().numpy()[0])\n",
    "    done = terminated or truncated # Atualiza 'done' corretamente\n",
    "\n",
    "    obs = torch.tensor(obs_next, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    step += 1\n",
    "    # env_render.render() # Não é necessário se render_mode=\"human\" já faz isso\n",
    "\n",
    "env_render.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
