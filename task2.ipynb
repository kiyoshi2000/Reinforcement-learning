{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f64ff1",
   "metadata": {},
   "source": [
    "# Policy Gradient on Racetrack Environment (Optimized)\n",
    "\n",
    "This notebook implements a Policy Gradient method for a custom racetrack environment using PyTorch. It includes:\n",
    "- Dynamic input size handling\n",
    "- Entropy-based regularization\n",
    "- Advantage normalization\n",
    "- Gradient clipping\n",
    "- **Performance optimization** by disabling rendering\n",
    "\n",
    "We use the REINFORCE algorithm, which is a Monte Carlo policy gradient method for optimizing the policy network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b3cec",
   "metadata": {},
   "source": [
    "### 1. Imports and Device Configuration\n",
    "\n",
    "We import the required libraries and set the device to CUDA if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b90b826",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Normal\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e7816",
   "metadata": {},
   "source": [
    "### 2. Training Mode and Render Settings\n",
    "\n",
    "We define a simple flag to control whether the environment should render for visualization or run headlessly for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86a84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True\n",
    "render_mode = None if training else \"human\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee153d",
   "metadata": {},
   "source": [
    "### 3. Environment Configuration\n",
    "\n",
    "The configuration dictionary defines both the observation and action spaces, along with various simulation and reward parameters. Key features include:\n",
    "- OccupancyGrid observation type with vehicle presence and road occupancy\n",
    "- Continuous action space with lateral-only control (no acceleration)\n",
    "- Reward structure penalizing collisions, drifting from the lane center, and overactive control\n",
    "- Rendering is disabled when training to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ae1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"OccupancyGrid\",\n",
    "        \"features\": ['presence', 'on_road'],\n",
    "        \"grid_size\": [[-18, 18], [-18, 18]],\n",
    "        \"grid_step\": [3, 3],\n",
    "        \"as_image\": False,\n",
    "        \"align_to_vehicle_axes\": True\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\",\n",
    "        \"longitudinal\": False,\n",
    "        \"lateral\": True\n",
    "    },\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"policy_frequency\": 5,\n",
    "    \"duration\": 100,\n",
    "    \"collision_reward\": -1,\n",
    "    \"lane_centering_cost\": 4,\n",
    "    \"action_reward\": -0.3,\n",
    "    \"controlled_vehicles\": 1,\n",
    "    \"other_vehicles\": 0,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 600,\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 7,\n",
    "    \"show_trajectories\": False,\n",
    "    \"offroad_terminal\": True,\n",
    "    \"render_agent\": False if training else True,\n",
    "    \"offscreen_rendering\": True if training else False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0006b",
   "metadata": {},
   "source": [
    "### 4. Environment Initialization\n",
    "\n",
    "We instantiate the racetrack environment and inject the configuration directly into the unwrapped environment. A sample observation is retrieved to infer the shape of the state space and action dimensionality for the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3bd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"racetrack-v0\", render_mode=render_mode)\n",
    "env.unwrapped.configure(config)\n",
    "obs_sample, _ = env.reset()\n",
    "\n",
    "obs_shape = obs_sample.shape\n",
    "act_dim = env.action_space.shape[0] if len(env.action_space.shape) > 0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b46710",
   "metadata": {},
   "source": [
    "### Policy and Value Networks\n",
    "\n",
    "We use a classic actor-critic architecture in which:\n",
    "- The Actor learns a stochastic policy modeled by a Normal distribution, allowing for continuous actions.\n",
    "- The Critic estimates the value function V(s), used to reduce the variance of the policy gradient via advantage estimation.\n",
    "- Both models use a fully connected feedforward network with ReLU activations and are designed to support dynamic input shapes.\n",
    "\n",
    "### Actor Network\n",
    "\n",
    "The actor outputs the parameters of a Normal distribution from which actions are sampled. We use log_std as a learnable parameter to allow the network to control its exploration behavior dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23f0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_shape, act_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Dynamically compute input size after flattening\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *obs_shape)\n",
    "            n_flatten = self.flatten(dummy).shape[1]\n",
    "\n",
    "        # Policy network (shared feature extractor)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_flatten, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Output layers: mean and log standard deviation\n",
    "        self.mean = nn.Linear(hidden_size, act_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(act_dim))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.flatten(obs)\n",
    "        x = self.net(x)\n",
    "        mu = self.mean(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        return Normal(mu, std)  # Return a distribution object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27890fe1",
   "metadata": {},
   "source": [
    "### Critic Network\n",
    "\n",
    "The critic estimates the value of each state using a separate feedforward network. Its output is a scalar value for each state input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61819b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_shape, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *obs_shape)\n",
    "            n_flatten = self.flatten(dummy).shape[1]\n",
    "\n",
    "        self.v = nn.Sequential(\n",
    "            nn.Linear(n_flatten, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.flatten(obs)\n",
    "        return self.v(x).squeeze(-1)  # Output shape: (batch,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa299c",
   "metadata": {},
   "source": [
    "### Model Initialization and Optimizers\n",
    "\n",
    "We instantiate the networks and their optimizers. AdamW is used for better weight decay handling compared to standard Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a09ccadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "actor = Actor(obs_shape, act_dim).to(device)\n",
    "critic = Critic(obs_shape).to(device)\n",
    "\n",
    "# Optimizers\n",
    "opt_actor = optim.AdamW(actor.parameters(), lr=1e-3)\n",
    "opt_critic = optim.AdamW(critic.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d32d73",
   "metadata": {},
   "source": [
    "## REINFORCE with Baseline: Training Loop\n",
    "\n",
    "This section implements the training procedure for a policy gradient method (REINFORCE) using the actor-critic architecture defined earlier.\n",
    "\n",
    "Key elements of the implementation:\n",
    "- The actor samples actions from a Normal distribution.\n",
    "- The critic estimates the value function V(s) to compute the advantage.\n",
    "- Rewards are accumulated using Monte Carlo returns.\n",
    "- The policy is optimized using the advantage-weighted log-probabilities.\n",
    "- We include entropy regularization to encourage exploration.\n",
    "- Gradients are clipped to stabilize training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "439614ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Return: 70.8\n",
      "Epoch 10 | Return: 119.5\n",
      "Epoch 20 | Return: 76.2\n",
      "Epoch 30 | Return: 92.8\n",
      "Epoch 40 | Return: 345.6\n",
      "Epoch 50 | Return: 552.3\n",
      "Epoch 60 | Return: 302.9\n",
      "Epoch 70 | Return: 696.2\n",
      "Epoch 80 | Return: 969.6\n",
      "Epoch 90 | Return: 955.0\n",
      "Epoch 100 | Return: 382.1\n",
      "Epoch 110 | Return: 725.8\n",
      "Epoch 120 | Return: 947.6\n",
      "Epoch 130 | Return: 982.5\n",
      "Epoch 140 | Return: 686.5\n",
      "Epoch 150 | Return: 922.7\n",
      "Epoch 160 | Return: 1034.3\n",
      "Epoch 170 | Return: 804.4\n",
      "Epoch 180 | Return: 848.9\n",
      "Epoch 190 | Return: 970.6\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99     # Discount factor for return calculation\n",
    "epochs = 200     # Number of episodes (training epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Reset environment\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Buffers for trajectory data\n",
    "    log_probs, values, rewards = [], [], []\n",
    "\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done and step < 100:\n",
    "        # Forward pass: policy and value\n",
    "        dist = actor(obs)                      # Returns Normal(mu, std)\n",
    "        value = critic(obs)                    # Scalar value prediction\n",
    "        action = dist.sample()                 # Sample from policy\n",
    "        log_prob = dist.log_prob(action).sum(-1)\n",
    "\n",
    "        # Environment transition\n",
    "        obs_next, reward, done, truncated, _ = env.step(action.cpu().numpy()[0])\n",
    "\n",
    "        # Save transition data\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # Move to next state\n",
    "        obs = torch.tensor(obs_next, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        step += 1\n",
    "\n",
    "    # Compute Monte Carlo returns\n",
    "    returns, G = [], 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "    values = torch.cat(values)\n",
    "\n",
    "    # Compute normalized advantages\n",
    "    advantages = returns - values\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "    entropy = dist.entropy().mean()\n",
    "\n",
    "    # Total loss includes entropy bonus and value loss with coefficient\n",
    "    loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "    # Gradient step\n",
    "    opt_actor.zero_grad()\n",
    "    opt_critic.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), 0.5)\n",
    "    torch.nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
    "    opt_actor.step()\n",
    "    opt_critic.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} | Return: {returns.sum().item():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb703707",
   "metadata": {},
   "source": [
    "### Rendering the Trained Agent\n",
    "\n",
    "After training the policy, we can visualize its performance by running the agent in the environment with rendering enabled. This helps qualitatively assess how well the agent learned the task, such as staying on track or making smooth lateral movements.\n",
    "\n",
    "#### Key Points\n",
    "- The environment is re-initialized with \"human\" render mode to display a GUI window.\n",
    "- The trained policy is used in deterministic mode by taking the mean of the action distribution.\n",
    "- Rendering is done for a limited number of steps or until the episode terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0be983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:31:32.434 Python[22396:489488] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "env_render = gym.make(\"racetrack-v0\", render_mode=\"human\")\n",
    "env_render.unwrapped.configure(config)\n",
    "\n",
    "obs, _ = env_render.reset()\n",
    "obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "while not done and step < 300:\n",
    "    dist = actor(obs)                                          # Forward pass\n",
    "    action = dist.mean                                          # Deterministic policy (mean of Normal)\n",
    "    obs_next, reward, done, truncated, _ = env_render.step(action.detach().cpu().numpy()[0])\n",
    "    \n",
    "    obs = torch.tensor(obs_next, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    step += 1\n",
    "\n",
    "env_render.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
