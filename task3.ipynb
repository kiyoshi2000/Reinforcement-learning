{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized configuration for the 'parking-v0' environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 1,\n",
    "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "        \"absolute\": True\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\"\n",
    "    },\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"policy_frequency\": 5,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 600,\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5.5,\n",
    "    \"render_agent\": True,\n",
    "    \"vehicles_count\": 1,\n",
    "    \"duration\": 100,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "# Creating the environment with the configuration\n",
    "env = gym.make(\"parking-v0\", render_mode=\"rgb_array\")\n",
    "env.unwrapped.configure(config)\n",
    "\n",
    "# Vectorizing the environment for stable training\n",
    "vec_env = make_vec_env(lambda: env, n_envs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with PPO\n",
    "\n",
    "Could not fix the problem below. Therefore, we could not check whether the next codes work, so we did it based on other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[64, 64], vf=[64, 64])]\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MultiInputPolicy\",\n",
    "    env=vec_env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_parking_tensorboard/\",\n",
    "    policy_kwargs=policy_kwargs\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Average reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an episode with rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()[0]\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total reward for the rendered episode: {total_reward:.2f}\")\n",
    "\n",
    "# Closing the environment \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
